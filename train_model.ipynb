{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Piano Arrangement\n",
    "\n",
    "This notebook processes audio files, generates spectrograms, trains a machine learning model, and generates an arrangement from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, TimeDistributed, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining directories\n",
    "BASE_DIR = './dataset/instrumental_only/'\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'spectrograms')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and save spectrogram\n",
    "def generate_mel_spectrogram(audio_path, save_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "    spectrogram_db = librosa.power_to_db(S, ref=np.max)\n",
    "    np.save(save_path, spectrogram_db)\n",
    "    return spectrogram_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display spectrogram\n",
    "def plot_spectrogram(spectrogram_db, title):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(spectrogram_db, x_axis='time', y_axis='mel', cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing audio files\n",
    "def process_audios(original_dir, piano_dir):\n",
    "    original_files = [f for f in os.listdir(original_dir) if f.endswith(('.mp3', '.wav'))]\n",
    "    piano_files = [f for f in os.listdir(piano_dir) if f.endswith(('.mp3', '.wav'))]\n",
    "\n",
    "    original_specs = []\n",
    "    piano_specs = []\n",
    "\n",
    "    for original_file, piano_file in zip(original_files, piano_files):\n",
    "        original_spec = generate_mel_spectrogram(os.path.join(original_dir, original_file), os.path.join(OUTPUT_DIR, 'original', f'{original_file}.npy'))\n",
    "        piano_spec = generate_mel_spectrogram(os.path.join(piano_dir, piano_file), os.path.join(OUTPUT_DIR, 'piano', f'{piano_file}.npy'))\n",
    "        original_specs.append(original_spec)\n",
    "        piano_specs.append(piano_spec)\n",
    "\n",
    "    return np.array(original_specs), np.array(piano_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model (CNN + LSTM)\n",
    "def build_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Reshape((*input_shape, 1), input_shape=input_shape),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Reshape((input_shape[0], -1)), \n",
    "        LSTM(128, return_sequences=True),\n",
    "        TimeDistributed(Dense(input_shape[1]))\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of spectrogram to audio\n",
    "def spectrogram_to_audio(spectrogram_db, sr=22050):\n",
    "    spectogram = librosa.db_to_power(spectrogram_db)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(spectogram, sr=sr)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input directories\n",
    "original_dir = os.path.join(BASE_DIR, 'original')\n",
    "piano_dir = os.path.join(BASE_DIR, 'piano')\n",
    "\n",
    "# Processing audio files\n",
    "original_specs, piano_specs = process_audios(original_dir, piano_dir)\n",
    "\n",
    "# Dividing data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(original_specs, piano_specs, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dividing data into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Checking the length of the data\n",
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and summarizing the model\n",
    "model = build_model(X_train.shape[1:])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating an arrangement\n",
    "predicted_spec = model.predict(X_test)\n",
    "generated_audio = spectrogram_to_audio(predicted_spec[0])\n",
    "\n",
    "# Saving the generated audio\n",
    "librosa.output.write_wav('generated_arrangement.wav', generated_audio, sr=22050)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
